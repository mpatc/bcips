<pre>
  BIP: ?
  Layer: Consensus (hard fork)
  Title: Safer and growing block size limit
  Author: mpatc <seoulbit@gmail.com>
  Comments-Summary: No comments yet.
  Comments-URI: FIXME
  Status: Draft
  Type: Standards Track
  Created: 2017-05-17
  License: BSD-2-Clause
</pre>

==Abstract==

The block size limit is gradually increased over time, eventually expanding beyond the current limits.

==Copyright==

This BIP is licensed under the BSD 2-clause license.

==Specification==

Upon activation, the block size limitation is replaced by the function below, applied to the median of the timestamps of the previous 11 blocks, or in code terms: the block size limit for pindexBlock is <code>GetMaxBlockSize(pindexBlock->pprev->GetMedianTimePast())</code>.

It implements a series of block size steps, one every ~97 days, and ending at just under 31 MB in 2045 April, with each step increasing the maximum block size by 4.4%, allowing an overall growth of 17.7% per year.
The initial size limit upon activation depends on when it is activated: for example, if in 2018 January, it would begin at ~356k; or if in 2024 June, it would begin at just over 1 MB.

<pre>
uint32_t GetMaxBlockSize(int64_t nMedianTimePast) {
    // In case we activate early on regtest.
    if (nMedianTimePast < 1483246800) {
        return 1000000;
    }
    // The first step is on January 1st 2017.
    // After that, one step happens every 2^23 seconds.
    int64_t step = (nMedianTimePast - 1483246800) >> 23;
    // Don't do more than 107 steps, to stay under 32 MB.
    step = std::min<int64_t>(step, 107);
    // Every step is a 2^(1/16) factor.
    static const uint32_t bases[16] = {
        // bases[i] == round(300000 * pow(2.0, i / 16.0))
        300000, 313282, 327152, 341637,
        356762, 372557, 389052, 406277,
        424264, 443048, 462663, 483147,
        504538, 526876, 550202, 574562
    };
    return bases[step & 15] << (step / 16);
}
</pre>

The concept of block weight is introduced or modified:
all witness data, including both scriptSig (pre-segwit transactions) and segwit witness data (if applicable), is measured as 1 weight-unit (WU), while all other data in the block is measured as 4 WU.
The weight of a block may not exceed double the size limit in bytes.

The maximum size of a transaction stripped of witness data is limited to 1 MB.

===Deployment===

This BIP is to be deployed in two stages:

====Stage 1====

The first stage is deploying in full node software deactivated. This requires consensus from the entire Bitcoin community, and ought to be evaluated by the release manager of each full node implementation before release. All full node implementations must deploy this change before 2024, or the proposal is deemed to have failed.

Since this stage only implies growth of the block size limit (not reduction), far in advance of its effective date, and should not in any way impede the efforts of others to increase (or decrease) it faster/sooner (eg, BIPs 10x) or even substantially alter the very concept of decentralised consensus (eg, BU), it is hoped this proposal will be uncontroversial and thus able to attain consensus from the entire community at the very least as a better failure scenario than the current "1 MB forever" (or "2-3 MB forever" with segwit).

====Stage 2====

After consensus is established (but not necessarily before all nodes have deployed compatible software, so long as this still occurs before 2024), the BIP may be activated using BIP 9 using the '''name''' TBD and '''bit''' TBD.

For Bitcoin mainnet, the BIP9 '''starttime''' will be TBD (Epoch timestamp TBD) and BIP9 '''timeout''' will be 2024 September TBD (Epoch timestamp TBD).

For Bitcoin testnet, the BIP9 '''starttime''' will be TBD (Epoch timestamp TBD) and BIP9 '''timeout''' will be TBD (Epoch timestamp TBD).

==Motivation==

Many people want to see Bitcoin scale over time, allowing an increasing number of transactions on the block chain. It would come at an increased cost for the ecosystem (bandwidth, processing, and storage for relay nodes, as well as an impact on propagation speed of blocks on the network), but technology also improves over time. When all technologies depended on have improved as well as their availability on the market, there is no reason why Bitcoin's fundamental transaction rate cannot improve proportionally.


While some users feel the current rate of blockchain growth is too high, efforts to deal with this belong in another BIP. Most users are now concerned with high feels.


Currently, there is a consensus rule in place that limits the size of blocks to 1000000 bytes. Changing this presently requires a hard-forking change: one that will require every full node in the network to implement the new rules. The new chain created by those changed nodes will be rejected by old nodes, so this would effectively be a request to the ecosystem to migrate to a new and incompatible network. Doing this while controversy exists is dangerous to the network and the ecosystem.
This BIP gives users plenty of time to upgrade.

Upgrading can be done even if this BIP does not activate for several years due to potential controversy over the short-term reductions, and as such consensus for its deployment should be determined only by the long-term block size increase.

==Rationale==

Won't sudden increases in the block size limit upset market forces?

* Because every increase (including the first) is only 4.4%, risk from large market or technological changes is minimized.


What is the purpose of the chosen growth rate?

* The growth rate of 17.7% growth per year is consistent with the average growth rate of bandwidth the last years, which seems to be an important bottleneck. If over time, the 17.7% annual block size growth factor is beyond what actual technology improvements offer, the intention should be to soft fork to a smaller block size limit.
* This growth rate leads to the new block size limit exceeding the current limit only after seven years, by which time existing nodes using BIP-FIXME:hfprep will relax their own block size rules, making this BIP a softfork rather than a hardfork.

Why is median time past metric used rather than block time?

* Using the "median time past" guarantees monotonic behaviour, as this median is required to be increasing, according to Bitcoin's existing consensus rules. Using the "median time past" of the block before means we know in advance what the limit of each block will be, without depending on the actual block's timestamp.


Why is the block weight limit chosen at twice the size limit?

* With a typical 2-of-2 multisig spend with 2 inputs and 2 outputs, the weight is approximately 80% higher than the size.

Isn't just the weight limit sufficient?

* Concerns exist that only a weight limit increases the resource requirements to handle spam blocks, while legitimate blocks only get to use a subset of the maximum block size for legitimate capacity. Enforcing a block size limit keeps the worst-case scenario closer to legitimate real-world usage.

==Backwards compatibility==

With this algorithm, the block size may exceed the current limits during 2024 June.
Several years is sufficient time for all nodes to update so long as this proposal's extension beyond current limits has consensus by then.

==Reference implementation==

TBA
